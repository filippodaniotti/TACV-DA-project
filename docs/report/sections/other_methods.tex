%author: Giovanni
\paragraph{Other methods}
% For completeness we analyze other methods that belongs or not to the prevoius classes. 
M-ADDA\cite[Issam H. et al.]{madda} is an adversarial deep metric learning where we use a metric to perform target clustering. The clusters are encoded by a decoder that maps the target features in an embedding space.\newline 
The training of the models is differentiated between the source model and target model. We use a triplet loss to optimize source network parameters by computing the square distance between a random chosen example and two other examples, one with the same label and one with different label. The target model is trained through two losses, one for the target encoder, which consists in an adversarial loss and one for the decoder, which is a magnet loss. The magnet loss computes simultaneously the distribution of each target class in the embedding space and reduces the overlap between clusters. \newline 
In Heuristic Domain Adaptation\cite[Shuhao Cui et al.]{HDA} they use a fundament network that learns the domain-invariance representation and an heuristic network that focuses on domain-specific features. 
We ideally want a model that has learned the distributions of the two domains without any domain specific bias. To ensure the convergence of the two networks they compute the cosine similarity between the generator function (difference of fundamental and heurstic functions) and the heuristic function itself.

Finally, in  \cite[Yunzhong H. et al.]{VAK} they investigate what neural networks learn in domain adaptation. They proposed a source-free image translation (SFIT), a novel method that generates source-style images from original target images through a generator network, so that it mitigates and represents the knowledge difference between models. The entire procedure is built around two main concepts, which are \emph{relationship preserving} and \emph{knowledge distillation}. Through knowledge distillation they distill the knowledge of the generator by portraying the adapted knowledge in the target model with source model and generator combined. Relationship preserving indicates a successful depiction of the target model knowledge on the generated images and more in general it states the distributions' alignment.