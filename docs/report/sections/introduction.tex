\section{Unsupervised domain adaptation}
In domain adaptation (DA) the goal is to minimize the domain loss that occurs when moving a model from a source, seen domain, $X_s$ to a target $X_t$, unseen domain by minimizing the \emph{domain shift}. In the particular case of unsupervised domain adaptation (UDA) only source labels are available in during training $X_s=\{x_s,y_s\}$, whereas target labels are not $X_t=\{x_t\}$. Here we present a non-comprehensive overview of the many methods available for UDA.

% In unsupervised domain adaptation (UDA) we train on a source \emph{labelled} dataset $X_s=\{x_s,y_s\}$ and then we test on a different, \emph{unlabelled} dataset $X_t=\{x_t\}$. As the two datasets are different, there is a \emph{domain shift} from $X_s$ to $X_t$; hence, the model performs worse on the target dataset. Most unsupervised domain adaptation techniques try to bridge this gap.
% 