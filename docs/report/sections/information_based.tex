%author: Andrea
% \section{Information-based methods}
\paragraph{Information-based methods}
Mutual Information (MI) maximization has been shown as a promising approach in unsupervised learning. In particular, we can define the Mutual information (MI) of two random variables as a measure of the mutual dependence between the two variables. Recently, it has also been used in the more restricted context of hypothesis transfer learning (HTL). Specifically, \cite[Lao et al.]{Lao2020} propose a model that use MI maximization on the unlabeled target data to transfer knowledge from a set of source hypothesis, learned from the source domain, to a corresponding target set of target hypotheses. This is in contrast to the common approaches for HTL and UDA that tend to use a single hypothesis, failing to uncover different modes of the model distribution.
The crucial drawback of multiple independent MI maximization is that different target hypotheses can be optimized in an unrestricted manner due to a lack of supervision. To overcome this limitation, an hypothesis disparity (HD) regularization is included to align the target hypothesis. This is done in a way that takes uncertainty expressed through different source hypothesis into consideration while marginalizing out the undesired disagreements.